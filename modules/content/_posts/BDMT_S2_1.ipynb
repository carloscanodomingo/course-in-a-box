{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b5a1e9a-5401-435d-9739-2ca677fcc764",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# ‚ú® Data Cleaning and Imputation for Missing Values ‚ú®\n",
    "\n",
    "In this notebook, we will handle missing data from a housing dataset. Specifically, we will identify missing values and impute them using an appropriate strategy (mode imputation for the `hotwaterheating` column).\n",
    "\n",
    "---\n",
    "\n",
    "## üìù 1. Load the Dataset\n",
    "\n",
    "Let's begin by loading the dataset and displaying a few records to get an overview of the data.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Housing_V0.csv'\n",
    "housing_data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "housing_data.head()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîç 2. Check for Missing Values\n",
    "\n",
    "Before performing any data imputation, it's essential to check for missing values in the dataset.\n",
    "\n",
    "```python\n",
    "# Check for any missing values\n",
    "null_values = housing_data.isnull().sum()\n",
    "\n",
    "# Display the columns with null values\n",
    "null_values[null_values > 0]\n",
    "```\n",
    "\n",
    "### üìä Output:\n",
    "\n",
    "```\n",
    "hotwaterheating    13\n",
    "```\n",
    "\n",
    "As we can see, the `hotwaterheating` column contains 13 missing values.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß 3. Impute Missing Values\n",
    "\n",
    "We will impute the missing values in the `hotwaterheating` column using the **mode**, which is the most frequent value in that column.\n",
    "\n",
    "```python\n",
    "# Impute missing values using the mode of 'hotwaterheating'\n",
    "mode_value = housing_data['hotwaterheating'].mode()[0]\n",
    "housing_data['hotwaterheating'].fillna(mode_value, inplace=True)\n",
    "\n",
    "# Verify if any missing values remain\n",
    "null_values_after_imputation = housing_data.isnull().sum()\n",
    "\n",
    "# Display the result\n",
    "null_values_after_imputation\n",
    "```\n",
    "\n",
    "### üìä Output:\n",
    "\n",
    "```\n",
    "No missing values remain in the dataset.\n",
    "```\n",
    "\n",
    "The missing values have been successfully filled using the most frequent value (mode).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 4. Conclusion\n",
    "\n",
    "We have successfully handled the missing values in the `hotwaterheating` column by applying mode imputation. The dataset is now clean and ready for further analysis or modeling.\n",
    "\n",
    "Next steps:\n",
    "- Explore the data further for outliers or inconsistencies.\n",
    "- Proceed with data transformation, scaling, or feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee6c9d-4190-4c4c-9481-d4e292ff6560",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# üö´ Removing Outliers Based on Price\n",
    "\n",
    "In this notebook, we will identify and remove outliers in the dataset based on the `price` column using the IQR (Interquartile Range) method.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Load the Dataset\n",
    "\n",
    "Let's begin by loading the dataset and displaying a few records to get an overview of the data.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Housing_V0.csv'\n",
    "housing_data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "housing_data.head()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Identify Outliers in `price`\n",
    "\n",
    "We will use the IQR method to identify outliers. Outliers are data points that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR.\n",
    "\n",
    "```python\n",
    "# Calculate Q1, Q3, and IQR for 'price'\n",
    "Q1 = housing_data['price'].quantile(0.25)\n",
    "Q3 = housing_data['price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the upper and lower bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify rows with outliers in the 'price' column\n",
    "outliers = housing_data[(housing_data['price'] < lower_bound) | (housing_data['price'] > upper_bound)]\n",
    "\n",
    "# Display the outliers\n",
    "outliers\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Remove Outliers from the Dataset\n",
    "\n",
    "After identifying the outliers, we will remove them from the dataset.\n",
    "\n",
    "```python\n",
    "# Remove outliers from the dataset\n",
    "housing_data_cleaned = housing_data[~((housing_data['price'] < lower_bound) | (housing_data['price'] > upper_bound))]\n",
    "\n",
    "# Verify the number of remaining rows\n",
    "housing_data_cleaned.shape\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Conclusion\n",
    "\n",
    "We have successfully removed the outliers from the dataset based on the `price` column. The dataset is now ready for further analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2142e67-9591-4f1c-9631-9d791c272672",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# üìè Scaling the 'Area' Column Between 0 and 1\n",
    "\n",
    "In this notebook, we will scale the `area` column values to fall between 0 and 1 using **Min-Max Scaling**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Load the Dataset\n",
    "\n",
    "Let's begin by loading the dataset and displaying a few records to get an overview of the data.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Housing_V0.csv'\n",
    "housing_data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "housing_data.head()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Apply Min-Max Scaling to the `area` Column\n",
    "\n",
    "We will use the `MinMaxScaler` from the `sklearn.preprocessing` module to scale the `area` values between 0 and 1.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the 'area' column\n",
    "housing_data[['area']] = scaler.fit_transform(housing_data[['area']])\n",
    "\n",
    "# Display the first few rows of the scaled dataset\n",
    "housing_data.head()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Verify the Scaling\n",
    "\n",
    "Check the minimum and maximum values of the `area` column to confirm that they are scaled between 0 and 1.\n",
    "\n",
    "```python\n",
    "# Verify the min and max values of the scaled 'area' column\n",
    "print(\"Min value:\", housing_data['area'].min())\n",
    "print(\"Max value:\", housing_data['area'].max())\n",
    "```\n",
    "\n",
    "### Expected Output:\n",
    "```\n",
    "Min value: 0.0\n",
    "Max value: 1.0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Conclusion\n",
    "\n",
    "The `area` column has been successfully scaled to fall between 0 and 1. The dataset is now ready for further analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06716864-228e-4d37-8490-a431be2aeea0",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# üîÑ One-Hot Encoding the 'Furnishingstatus' Column\n",
    "\n",
    "In this notebook, we will apply **one-hot encoding** to the `furnishingstatus` column, which is a categorical variable, to convert it into multiple binary columns.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Load the Dataset\n",
    "\n",
    "Let's begin by loading the dataset and displaying a few records to get an overview of the data.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Housing_V0.csv'\n",
    "housing_data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "housing_data.head()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Apply One-Hot Encoding to the `furnishingstatus` Column\n",
    "\n",
    "We will use the `get_dummies` method from pandas to apply one-hot encoding to the `furnishingstatus` column.\n",
    "\n",
    "```python\n",
    "# Apply one-hot encoding to 'furnishingstatus'\n",
    "housing_data_encoded = pd.get_dummies(housing_data, columns=['furnishingstatus'], drop_first=True)\n",
    "\n",
    "# Display the first few rows of the encoded dataset\n",
    "housing_data_encoded.head()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Verify the Encoding\n",
    "\n",
    "Check that the `furnishingstatus` column has been converted into binary columns.\n",
    "\n",
    "```python\n",
    "# Display the columns to verify one-hot encoding\n",
    "housing_data_encoded.columns\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Conclusion\n",
    "\n",
    "The `furnishingstatus` column has been successfully converted using one-hot encoding. The dataset is now ready for further analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2202cc-9e0e-48b3-a005-64b8931072d0",
   "metadata": {},
   "source": [
    "\n",
    "# ‚úÇÔ∏è Splitting the Dataset into Training and Testing Sets\n",
    "\n",
    "In this notebook, we will split the dataset into **training** and **testing** sets to prepare it for model training and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Load the Dataset\n",
    "\n",
    "Let's begin by loading the dataset and displaying a few records to get an overview of the data.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Housing_V0.csv'\n",
    "housing_data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "housing_data.head()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Split the Data into Training and Testing Sets\n",
    "\n",
    "We will use the `train_test_split` function from `sklearn.model_selection` to split the data. Typically, we split 80% for training and 20% for testing.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the feature set and target variable\n",
    "X = housing_data.drop(columns=['price'])  # Example: using 'price' as the target variable\n",
    "y = housing_data['price']\n",
    "\n",
    "# Split the dataset (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(\"Training set shape (X_train, y_train):\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape (X_test, y_test):\", X_test.shape, y_test.shape)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Verify the Data Split\n",
    "\n",
    "We will check the shapes of the training and testing sets to ensure that the split was successful.\n",
    "\n",
    "```python\n",
    "# Verify the first few rows of the training set\n",
    "X_train.head()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Conclusion\n",
    "\n",
    "The dataset has been successfully split into training and testing sets. These sets can now be used for training machine learning models and evaluating their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e836b324-7e4e-4b7b-815f-b839c98ce19c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
